{"posts":[{"title":"WWW Paper work summary","content":"We focused on the close world data. Data collection Using the web browser automation tool and the data-network packet analyzer to automatically visit four highly popular social media websites and record network packets. We collect the data of totally 300 pages and 50 instances of each page. Data pre-processing Pre-processed data including removing outliers which include data with loading errors or large deviations from the packet size. Convert raw data to k-FP and CUMUL format. Identified all CDN traffic and resolved CDN URLs to IP addresses. Reproduce the experiments of k-FP and CUMUL k-FP Transfer the raw data to the format of Time and Direction. Extract k-FP features. Evaluate using random forest. CUMUL Transfer the raw data to the format of Time and Length. Extract CUMUL features. Divide CUMUL features into those for training data and testing data. Evaluate using SVM. Image Burst Extracted Image Burst features. Processed the features to the format which conform to random forest. Evaluate the model. Added one more cumulative burst size feature and re-evaluate the model. Information leakage Calculated the information leakage ( We formalized ) of Image Burst feature, k-FP feature and CUMUL feature. Latency Generate the latency data which conforms to gamma distribution. Added it to n samples in each instance. For verifying our approach’s validity, get the influence of the latency on CDN Burst with different alpha in gamma distribution. For defense, verify whether adding different mean values of delay can defend against attacks. Remove packets Remove n packets in each 100 packets. For defense, verify whether removing the packets can defend against attacks. Dummy packets Extracted the packets which don’t are in defense dataset as dummy packets. For defense, add dummy packets to known traces to defend our approach. ","link":"https://jzhzh.github.io/post/www-paper-work-summary/"}]}